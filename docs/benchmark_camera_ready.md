# Camera-Ready Benchmark Campaign Workflow

This document describes the config-driven campaign workflow for generating
camera-ready benchmark outputs across multiple planners.

## Entry Point

Run the campaign CLI:

```bash
uv run python scripts/tools/run_camera_ready_benchmark.py \
  --config configs/benchmarks/camera_ready_all_planners.yaml
```

Preflight-only (validate + preview + matrix summary, no episode execution):

```bash
uv run python scripts/tools/run_camera_ready_benchmark.py \
  --config configs/benchmarks/paper_experiment_matrix_v1.yaml \
  --mode preflight \
  --label preflight
```

Optional:

```bash
uv run python scripts/tools/run_camera_ready_benchmark.py \
  --config configs/benchmarks/camera_ready_all_planners.yaml \
  --label draft \
  --log-level INFO
```

For long/full campaigns, force worker log noise down:

```bash
LOGURU_LEVEL=INFO uv run python scripts/tools/run_camera_ready_benchmark.py \
  --config configs/benchmarks/camera_ready_all_planners.yaml \
  --label full_run
```

## Current Baseline (2026-02-20)

Current promoted all-planners baseline run:

- campaign id:
  - `camera_ready_all_planners_prediction_first_prediction_first_stop_verify_20260220_201848`
- root:
  - `output/benchmarks/camera_ready/camera_ready_all_planners_prediction_first_prediction_first_stop_verify_20260220_201848`
- status:
  - `8/8 planners ok`, `1080 episodes`, runtime `386.54s`
- predictive planner:
  - `status=ok`, `episodes=135`, `failed_jobs=0`
  - `success_mean=0.9778`, `collisions_mean=0.0000`, `near_misses_mean=0.0296`

## Config Presets

- `configs/benchmarks/camera_ready_smoke_all_planners.yaml`
  - single scenario smoke for fast validation
- `configs/benchmarks/camera_ready_baseline_safe.yaml`
  - baseline-ready planners on full scenario suite
- `configs/benchmarks/camera_ready_all_planners.yaml`
  - baseline + experimental planners on full scenario suite
  - prediction planner runs first for early fail-fast signal
  - `stop_on_failure: true` (aborts on `failed` and `partial-failure`)
- `configs/benchmarks/camera_ready_all_planners_strict_socnav.yaml`
  - full suite with strict SocNav prereq policy (`fail-fast`, no fallback)
- `configs/benchmarks/paper_experiment_matrix_v1.yaml`
  - frozen paper-facing execution contract (`paper_profile_version=paper-matrix-v1`)
  - mixed planner matrix with explicit `planner_group` tags (`core|experimental`)
  - differential-drive-only kinematics for v1 paper freeze
- `configs/algos/prediction_planner_camera_ready.yaml`
  - explicit `prediction_planner` camera-ready profile used by all-planners presets
  - resolves checkpoint via `predictive_model_id` from `model/registry.yaml`

`prediction_planner` is now part of all-planners campaign presets as an experimental planner.
For reproducible runs, verify that the configured model id exists and resolves to a valid local checkpoint before launching the campaign.

SNQI calibration assets used by camera-ready presets:

- `configs/benchmarks/snqi_weights_camera_ready_v1.json`
- `configs/benchmarks/snqi_baseline_camera_ready_v1.json`

## Produced Artifacts

Campaign outputs are written under:

`output/benchmarks/camera_ready/<campaign_id>/`

Expected tree:

```text
<campaign_id>/
  campaign_manifest.json
  manifest.json
  run_meta.json
  preflight/
    validate_config.json
    preview_scenarios.json
  runs/
    <planner_key>/
      episodes.jsonl
      summary.json
  reports/
    matrix_summary.json
    matrix_summary.csv
    amv_coverage_summary.json
    amv_coverage_summary.md
    campaign_summary.json
    campaign_table.csv
    campaign_table.md
    campaign_table_core.csv
    campaign_table_core.md
    campaign_table_experimental.csv
    campaign_table_experimental.md
    campaign_report.md
```

Publication bundle export is written under:

`output/benchmarks/publication/`

with files generated by `export_publication_bundle`.


## Campaign Summary Semantics

`reports/campaign_summary.json` contains:

- campaign metadata/provenance (scenario hash, git hash, runtime)
- per-planner run summary from benchmark runner
- per-planner aggregate statistics (mean and CI when available)
- flattened planner comparison rows
- matrix definition summary rows (`reports/matrix_summary.{json,csv}`)
- AMV scope coverage summary (`reports/amv_coverage_summary.{json,md}`)
- warning list
- publication bundle paths (if export enabled)
- interpretation profile metadata (`paper_interpretation_profile`)

## Reproducibility Metadata

Each campaign now stores the exact invocation and timing provenance.

Captured fields include:

- exact command used to launch the campaign (`invoked_command`)
- campaign wallclock start/end (`started_at_utc`, `finished_at_utc`)
- campaign runtime and throughput (`runtime_sec`, `episodes_per_second`)
- per-planner start/end/runtime/throughput in run entries and planner summaries
- seed-policy provenance (`mode`, configured seeds, resolved seed list)
- preflight artifact paths (`validate_config`, `preview_scenarios`)

Primary locations:

- `output/benchmarks/camera_ready/<campaign_id>/reports/campaign_summary.json`
  - `campaign.invoked_command`
  - `campaign.started_at_utc`
  - `campaign.finished_at_utc`
  - `campaign.runtime_sec`
  - `campaign.episodes_per_second`
  - `runs[].started_at_utc`
  - `runs[].finished_at_utc`
  - `runs[].runtime_sec`
  - `runs[].summary.episodes_per_second`
- `output/benchmarks/camera_ready/<campaign_id>/campaign_manifest.json`
  - `invoked_command`
  - `started_at_utc`
  - `runtime_sec`
- `output/benchmarks/camera_ready/<campaign_id>/run_meta.json`
  - `invoked_command`
  - `started_at_utc`
  - `finished_at_utc`
  - `runtime_sec`
  - `episodes_per_second`
  - `seed_policy.*`
  - `preflight_artifacts.*`
- `output/benchmarks/camera_ready/<campaign_id>/preflight/validate_config.json`
- `output/benchmarks/camera_ready/<campaign_id>/preflight/preview_scenarios.json`
- `output/benchmarks/camera_ready/<campaign_id>/reports/matrix_summary.json`
- `output/benchmarks/camera_ready/<campaign_id>/reports/matrix_summary.csv`
- `output/benchmarks/camera_ready/<campaign_id>/reports/campaign_report.md`
  - command in header
  - per-planner timing columns in the summary table

Quick inspection example:

```bash
jq '.campaign | {invoked_command, started_at_utc, finished_at_utc, runtime_sec, episodes_per_second}' \
  output/benchmarks/camera_ready/<campaign_id>/reports/campaign_summary.json
```

Inspect frozen matrix summary:

```bash
jq '.rows[0] | {planner_key, planner_group, kinematics, repeats, paper_profile_version}' \
  output/benchmarks/camera_ready/<campaign_id>/reports/matrix_summary.json
```

Analyzer helper:

```bash
uv run python scripts/tools/analyze_camera_ready_campaign.py \
  --campaign-root output/benchmarks/camera_ready/<campaign_id>
```

This emits:

- `reports/campaign_analysis.json`
- `reports/campaign_analysis.md`

The analyzer now also emits runtime hotspot diagnostics:

- top slow planners by campaign runtime
- per-planner `wall_time_sec` mean/p95
- top slow scenarios per hotspot planner

Campaign-to-campaign comparison helper:

```bash
uv run python scripts/tools/compare_camera_ready_campaigns.py \
  --base-campaign-root output/benchmarks/camera_ready/<base_campaign_id> \
  --candidate-campaign-root output/benchmarks/camera_ready/<candidate_campaign_id> \
  --output-json output/benchmarks/camera_ready/<candidate_campaign_id>/reports/campaign_comparison.json \
  --output-md output/benchmarks/camera_ready/<candidate_campaign_id>/reports/campaign_comparison.md
```

Use this to validate quality changes (for example predictive planner success/collision deltas)
after compatibility or config fixes.

Analyzer findings now also include portability checks, including detection of
absolute `scenario_params.map_file` paths in episodes (these should be
repository-relative for publication-grade portability).

## Camera-Ready Table Fields

`campaign_table.csv` and `campaign_table.md` include at least:

- planner key and algorithm
- execution mode and readiness status (`native` / `adapter` / `fallback` / `degraded`)
- readiness tier and preflight status
- episode count and failure count
- success/collision/near-miss means
- time-to-goal normalization mean
- path efficiency mean
- comfort exposure mean
- jerk mean
- SNQI mean and CI fields (if available)

Core vs experimental partitions:

- paper-facing profile (`paper_profile_version=paper-matrix-v1`):
  partitioning follows explicit planner tags from config (`planner_group=core|experimental`)
  as part of the frozen execution contract.
- non-paper runs:
  partitioning remains readiness-tier based.
- `campaign_table_core.{csv,md}`:
  core partition rows (`planner_group=core` for paper-facing runs;
  `readiness_tier=baseline-ready` for non-paper runs).
- `campaign_table_experimental.{csv,md}`:
  non-core rows (`planner_group!=core` for paper-facing runs; non-baseline-ready otherwise).

Portability guarantee:

- Episode `scenario_params.map_file` is normalized to repository-relative paths
  when the map resides in the repository tree (for example
  `maps/svg_maps/classic_crossing.svg`).

Additional diagnostics generated per campaign:

- `reports/scenario_breakdown.csv` and `reports/scenario_breakdown.md`
  - per-planner, per-scenario metric means
- `reports/scenario_family_breakdown.csv` and `reports/scenario_family_breakdown.md`
  - per-planner, per-family (archetype) metric means

## Notes on Experimental Planners

Experimental planners are executed with explicit profile and prereq policy from
the campaign config. For dependency-sensitive planners (for example SocNav
adapters), set `socnav_missing_prereq_policy: fallback` when you want campaign
execution to continue with degraded behavior instead of hard-fail.

Current planner mapping in map-runner:

- `socnav_sampling`: in-repo `SamplingPlannerAdapter` baseline (no upstream SocNavBench dependency).
- `socnav_bench`: upstream `SocNavBenchSamplingAdapter` wrapper (requires SocNav prereqs).

Decision rule for publication:

- Use strict profile/config when you need publication claims without degraded
  fallback behavior.
- Use fallback profile/config for diagnostics/exploration only, and always cite
  `preflight_status` plus the report disclosure section
  `SocNav Strict-vs-Fallback Disclosure`.

## Current Validation Snapshot

Validated on branch `codex/benchmark-camera-ready-pipeline`:

- baseline-safe calibration campaign (multi-seed `eval` set):
  - `camera_ready_baseline_safe_snqi_calib_base_20260217_122711`
  - `total_runs=3`, `successful_runs=3`, `total_episodes=405`
  - output used to derive:
    - `configs/benchmarks/snqi_baseline_camera_ready_v1.json`
- baseline-safe verification with SNQI enabled:
  - `camera_ready_baseline_safe_snqi_verify_20260217_123159`
  - `total_runs=3`, `successful_runs=3`, `total_episodes=405`
  - `snqi_mean` is numeric in campaign table (no `nan`)
- smoke all-planners with SNQI calibration enabled:
  - `camera_ready_smoke_all_planners_snqi_check_20260217_123000`
  - `total_runs=7`, `successful_runs=7`, `total_episodes=7`
  - `snqi_mean` is numeric in campaign table (no `nan`)
- full all-planners with multi-seed + SNQI enabled:
  - `camera_ready_all_planners_snqi_multiseed_verify_20260217_123437`
  - `total_runs=7`, `successful_runs=7`, `total_episodes=945`
  - `snqi_mean` is numeric for all planners in campaign table
- smoke all-planners:
  - `camera_ready_smoke_all_planners_smoke3_20260217_112307`
  - `total_runs=7`, `successful_runs=7`, `total_episodes=7`
- full all-planners:
  - `camera_ready_all_planners_full2_20260217_112600`
  - `total_runs=7`, `successful_runs=7`, `total_episodes=315`
  - campaign runtime: `130.03s`
  - publication bundle created

Artifact locations:

- campaign root:
  - `output/benchmarks/camera_ready/camera_ready_all_planners_snqi_multiseed_verify_20260217_123437`
- publication bundle:
  - `output/benchmarks/publication/camera_ready_all_planners_snqi_multiseed_verify_20260217_123437_publication_bundle`

## Remaining Camera-Ready Gaps

The pipeline is complete and reproducible, but final publication-grade reporting
still requires:

- release metadata finalization:
  - replace `release_tag`/DOI placeholders in campaign config before archival
