# Example PPO baseline configuration for robot_sf_bench
#
# Usage:
#   robot_sf_bench run \
#     --matrix path/to/matrix.yaml \
#     --out output/results/episodes.jsonl \
#     --algo ppo \
#     --algo-config configs/baselines/ppo.yaml
#
# Notes:
# - model_path: Path to a Stable-Baselines3 PPO .zip file. The default below
#   matches a tracked model in this repo; change as needed.
# - obs_mode: "vector" uses derived features (relative goal, velocity, nearest-K
#   pedestrian offsets). Set to "image" if your PPO policy expects image inputs
#   provided under obs.robot["image"].
# - action_space: "velocity" returns {vx, vy}; "unicycle" returns {v, omega}.
# - device: "auto" | "cpu" | "cuda" | "cuda:0" etc.

model_path: model/ppo_model_retrained_10m_2025-02-01.zip

# Device selection for SB3
device: auto
# Use deterministic=True for evaluation/benchmarking
deterministic: true

# Observation handling
obs_mode: vector # vector | image
nearest_k: 5 # number of nearest pedestrians to encode (vector mode)

# Action formatting
action_space: velocity # velocity | unicycle
v_max: 2.0
omega_max: 1.0

# Robustness: if model fails to predict (e.g., missing file or shape mismatch),
# fall back to a simple goal-seeking action so runs continue.
fallback_to_goal: true
