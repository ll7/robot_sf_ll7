# Default expert PPO training configuration generated for imitation workflows.
# This file captures deterministic seeds, scenario configuration, and
# convergence expectations so the expert pipeline can run reproducibly.
policy_id: ppo_expert_reference
scenario_config: ../../scenarios/classic_interactions.yaml
seeds:
  - 123
  - 231
  - 777
  - 992
total_timesteps: 100000
convergence:
  success_rate: 0.9
  collision_rate: 0.05
  plateau_window: 2000
evaluation:
  frequency_episodes: 10
  evaluation_episodes: 5
  hold_out_scenarios:
    - classic_crossing_medium
    - classic_bottleneck_high
# Optional canonical SNQI inputs used for eval metric logging.
# snqi_weights: ../../model/snqi_canonical_weights_v1.json
# snqi_baseline: ../../output/benchmarks/classic_social_navigation/baseline_stats.json
# Optional reward override passed through make_robot_env.
# env_factory_kwargs:
#   reward_name: snqi_step
#   reward_kwargs:
#     terminal_bonus: 0.5
