# Imitation Results Analysis

This guide explains how to generate a research-ready comparison between a baseline PPO training run and a pretrained run. The analysis script loads the training manifests, computes sample efficiency and performance deltas, renders figures, and emits a `training_summary.schema.json`-compatible summary.

## What it does

- Loads the baseline and pretrained manifests (falling back to prefixed/nested files if needed)
- Computes timesteps-to-convergence and sample-efficiency ratio
- Compares success rate, collision rate, and SNQI means
- Generates comparison figures (timesteps, performance bars)
- Writes `summary.json` and `summary.md` compliant with `contracts/training_summary.schema.json`

## Usage

```bash
uv run python scripts/tools/analyze_imitation_results.py \
  --group demo-comparison \
  --baseline ppo_baseline_runid \
  --pretrained ppo_finetune_runid \
  --output output/benchmarks/ppo_imitation/analysis/demo-comparison
```

### Arguments

- `--group` (required): Identifier for the comparison; becomes the `run_id` in the summary.
- `--baseline` (required): Baseline training run ID (manifest under `output/benchmarks/ppo_imitation/runs/`).
- `--pretrained` (required): Pretrained/fine-tuned training run ID.
- `--output` (optional): Output directory. Defaults to `output/benchmarks/ppo_imitation/analysis/<group>`.

### Outputs

- `summary.json` and `summary.md` in the output directory (schema-compatible)
- `figures/timesteps_comparison.png`
- `figures/performance_metrics.png`

### Notes

- Hardware in the summary is taken from the training manifests when present; otherwise it is marked as unknown (analysis host hardware is not recorded).
- Ensure training manifests exist (generated by `train_expert_ppo.py`, `pretrain_from_expert.py`, or `train_ppo_with_pretrained_policy.py`) before running the analysis.

## Generate a research report

After analysis, render a publication-ready report (Markdown + optional LaTeX):

```bash
uv run python scripts/research/generate_imitation_report.py \
  --summary output/benchmarks/ppo_imitation/analysis/demo-comparison/summary.json \
  --experiment-name imitation-study \
  --ablation-label "bc_epochs=5" \
  --hparam bc_epochs=5 --hparam dataset_size=5000 \
  --export-latex
```

Outputs under `output/imitation_reports/<experiment-name>_<run-id>/`:
- `report.md` (+ optional `report.tex`)
- `figures/` (reuses analysis figures)
- `data/summary.json`
- `metadata.json` with git/hardware/package info
- Optional ablation label and hyperparameters recorded when passed via CLI (`--ablation-label`, `--hparam key=value`)

### CLI flags (reports)
- `--ablation-label`: Free-form label to tag an ablation condition (e.g., dataset size, BC epochs).
- `--hparam key=value`: Repeatable; records hyperparameters in the report (Markdown + LaTeX).
