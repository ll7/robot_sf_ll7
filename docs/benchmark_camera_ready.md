# Camera-Ready Benchmark Campaign Workflow

This document describes the config-driven campaign workflow for generating
camera-ready benchmark outputs across multiple planners.

## Entry Point

Run the campaign CLI:

```bash
uv run python scripts/tools/run_camera_ready_benchmark.py \
  --config configs/benchmarks/camera_ready_all_planners.yaml
```

Optional:

```bash
uv run python scripts/tools/run_camera_ready_benchmark.py \
  --config configs/benchmarks/camera_ready_all_planners.yaml \
  --label draft \
  --log-level INFO
```

For long/full campaigns, force worker log noise down:

```bash
LOGURU_LEVEL=INFO uv run python scripts/tools/run_camera_ready_benchmark.py \
  --config configs/benchmarks/camera_ready_all_planners.yaml \
  --label full_run
```

## Config Presets

- `configs/benchmarks/camera_ready_smoke_all_planners.yaml`
  - single scenario smoke for fast validation
- `configs/benchmarks/camera_ready_baseline_safe.yaml`
  - baseline-ready planners on full scenario suite
- `configs/benchmarks/camera_ready_all_planners.yaml`
  - baseline + experimental planners on full scenario suite

## Produced Artifacts

Campaign outputs are written under:

`output/benchmarks/camera_ready/<campaign_id>/`

Expected tree:

```text
<campaign_id>/
  campaign_manifest.json
  manifest.json
  run_meta.json
  runs/
    <planner_key>/
      episodes.jsonl
      summary.json
  reports/
    campaign_summary.json
    campaign_table.csv
    campaign_table.md
    campaign_report.md
```

Publication bundle export is written under:

`output/benchmarks/publication/`

with files generated by `export_publication_bundle`.

## Campaign Summary Semantics

`reports/campaign_summary.json` contains:

- campaign metadata/provenance (scenario hash, git hash, runtime)
- per-planner run summary from benchmark runner
- per-planner aggregate statistics (mean and CI when available)
- flattened planner comparison rows
- warning list
- publication bundle paths (if export enabled)

## Camera-Ready Table Fields

`campaign_table.csv` and `campaign_table.md` include at least:

- planner key and algorithm
- episode count and failure count
- success/collision/near-miss means
- time-to-goal normalization mean
- path efficiency mean
- comfort exposure mean
- jerk mean
- SNQI mean and CI fields (if available)

## Notes on Experimental Planners

Experimental planners are executed with explicit profile and prereq policy from
the campaign config. For dependency-sensitive planners (for example SocNav
adapters), set `socnav_missing_prereq_policy: fallback` when you want campaign
execution to continue with degraded behavior instead of hard-fail.

## Current Validation Snapshot

Validated on branch `codex/benchmark-camera-ready-pipeline`:

- smoke all-planners:
  - `camera_ready_smoke_all_planners_smoke3_20260217_112307`
  - `total_runs=7`, `successful_runs=7`, `total_episodes=7`
- full all-planners:
  - `camera_ready_all_planners_full2_20260217_112600`
  - `total_runs=7`, `successful_runs=7`, `total_episodes=315`
  - campaign runtime: `130.03s`
  - publication bundle created

Artifact locations:

- campaign root:
  - `output/benchmarks/camera_ready/camera_ready_all_planners_full2_20260217_112600`
- publication bundle:
  - `output/benchmarks/publication/camera_ready_all_planners_full2_20260217_112600_publication_bundle`

## Remaining Camera-Ready Gaps

The pipeline is complete and reproducible, but final publication-grade reporting
still requires:

- calibrated SNQI setup:
  - provide canonical `snqi_weights` and baseline stats in config so `snqi_mean`
    is no longer `nan` in comparison tables
- multi-seed campaign presets:
  - use fixed seed sets (`seed_policy.mode: seed-set`) for stronger confidence
    intervals than single-seed runs
- release metadata finalization:
  - replace `release_tag`/DOI placeholders in campaign config before archival
