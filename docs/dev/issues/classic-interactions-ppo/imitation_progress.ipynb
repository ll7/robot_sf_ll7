{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "853c9cdd",
   "metadata": {},
   "source": [
    "# Classic Interactions PPO Progress\n",
    "\n",
    "This notebook inspects manifests/episode logs from the imitation pipeline to quantify sample efficiency; run all cells top-to-bottom after the latest training runs finish so plots reflect current data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa01ef50",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "- Baseline manifest: `output/benchmarks/ppo_imitation/runs/ppo_expert_reference_20251120T115451.json`\n",
    "- Pretrained manifest: `output/benchmarks/ppo_imitation/runs/ppo_finetune_finetuned_ppo_expert_reference.json`\n",
    "- Episode logs: `output/benchmarks/ppo_imitation/episodes/ppo_expert_reference_20251120T115451.jsonl` (baseline) — add matching fine-tune logs if they are captured later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35655c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available manifests (latest 5):\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "BASE_DIR = Path(\"output/benchmarks/ppo_imitation\")\n",
    "RUN_DIR = BASE_DIR / \"runs\"\n",
    "EPISODE_DIR = BASE_DIR / \"episodes\"\n",
    "BASELINE_RUN_ID = \"ppo_expert_reference_20251120T140528\"\n",
    "PRETRAINED_RUN_ID = \"ppo_finetune_finetuned_ppo_expert_reference\"\n",
    "\n",
    "print(\"Available manifests (latest 5):\")\n",
    "for stem in sorted(p.stem for p in RUN_DIR.glob(\"*.json\"))[-5:]:\n",
    "    print(\" -\", stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "486356c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Manifest output/benchmarks/ppo_imitation/runs/ppo_expert_reference_20251120T115451.json missing. Update BASELINE_RUN_ID/PRETRAINED_RUN_ID or run the pipeline first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m manifest_path.open(encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m json.load(handle)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m baseline_manifest = \u001b[43mload_manifest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASELINE_RUN_ID\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m pretrained_manifest = load_manifest(PRETRAINED_RUN_ID)\n\u001b[32m     14\u001b[39m baseline_manifest, pretrained_manifest\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mload_manifest\u001b[39m\u001b[34m(run_id)\u001b[39m\n\u001b[32m      4\u001b[39m     available = \u001b[38;5;28msorted\u001b[39m(p.stem \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m RUN_DIR.glob(\u001b[33m\"\u001b[39m\u001b[33m*.json\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      5\u001b[39m     hint = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAvailable manifests:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m- \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m- \u001b[39m\u001b[33m\"\u001b[39m.join(available[-\u001b[32m5\u001b[39m:]) \u001b[38;5;28;01mif\u001b[39;00m available \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m      7\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mManifest \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanifest_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m missing. Update BASELINE_RUN_ID/PRETRAINED_RUN_ID or run the pipeline first.\u001b[39m\u001b[33m\"\u001b[39m + hint\n\u001b[32m      8\u001b[39m     )\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m manifest_path.open(encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json.load(handle)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Manifest output/benchmarks/ppo_imitation/runs/ppo_expert_reference_20251120T115451.json missing. Update BASELINE_RUN_ID/PRETRAINED_RUN_ID or run the pipeline first."
     ]
    }
   ],
   "source": [
    "def load_manifest(run_id: str) -> dict:\n",
    "    \"\"\"Load a benchmark manifest by ID.\n",
    "\n",
    "    Args:\n",
    "        run_id: Manifest identifier used to build the filename.\n",
    "\n",
    "    Returns:\n",
    "        dict: Loaded manifest contents.\n",
    "    \"\"\"\n",
    "    manifest_path = RUN_DIR / f\"{run_id}.json\"\n",
    "    if not manifest_path.exists():\n",
    "        available = sorted(p.stem for p in RUN_DIR.glob(\"*.json\"))\n",
    "        hint = \"\\nAvailable manifests:\\n- \" + \"\\n- \".join(available[-5:]) if available else \"\"\n",
    "        raise FileNotFoundError(\n",
    "            f\"Manifest {manifest_path} missing. Update BASELINE_RUN_ID/PRETRAINED_RUN_ID or run the pipeline first.\"\n",
    "            + hint\n",
    "        )\n",
    "    with manifest_path.open(encoding=\"utf-8\") as handle:\n",
    "        return json.load(handle)\n",
    "\n",
    "\n",
    "baseline_manifest = load_manifest(BASELINE_RUN_ID)\n",
    "pretrained_manifest = load_manifest(PRETRAINED_RUN_ID)\n",
    "baseline_manifest, pretrained_manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d4c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_episode_dataframe(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load episode records from a JSONL file into a dataframe.\n",
    "\n",
    "    Args:\n",
    "        path: Path to the JSONL episode file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Aggregated episode records.\n",
    "    \"\"\"\n",
    "    records: list[dict] = []\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame()\n",
    "    with path.open(encoding=\"utf-8\") as handle:\n",
    "        for line in handle:\n",
    "            payload = json.loads(line)\n",
    "            row = {\n",
    "                \"episode\": payload.get(\"episode\"),\n",
    "                \"steps\": payload.get(\"steps\"),\n",
    "                \"seed\": payload.get(\"seed\"),\n",
    "            }\n",
    "            metrics = payload.get(\"metrics\", {})\n",
    "            for key, value in metrics.items():\n",
    "                row[f\"metric_{key}\"] = value\n",
    "            records.append(row)\n",
    "    return pd.DataFrame.from_records(records)\n",
    "\n",
    "\n",
    "baseline_episode_df = load_episode_dataframe(EPISODE_DIR / f\"{BASELINE_RUN_ID}.jsonl\")\n",
    "pretrained_episode_df = load_episode_dataframe(EPISODE_DIR / f\"{PRETRAINED_RUN_ID}.jsonl\")\n",
    "baseline_episode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"run_id\": BASELINE_RUN_ID,\n",
    "            \"type\": \"expert_baseline\",\n",
    "            \"timesteps\": baseline_manifest.get(\"notes\", [])[-1].split()[2]\n",
    "            if baseline_manifest.get(\"notes\")\n",
    "            else baseline_manifest.get(\"total_timesteps\", 0),\n",
    "        },\n",
    "        {\n",
    "            \"run_id\": PRETRAINED_RUN_ID,\n",
    "            \"type\": \"bc+ppo_finetune\",\n",
    "            \"timesteps\": pretrained_manifest.get(\"notes\", [])[-1].split()[2]\n",
    "            if pretrained_manifest.get(\"notes\")\n",
    "            else pretrained_manifest.get(\"total_timesteps\", 0),\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "summary_df[\"timesteps\"] = summary_df[\"timesteps\"].astype(int)\n",
    "summary_df[\"sample_eff_ratio\"] = (\n",
    "    summary_df.loc[summary_df[\"type\"] == \"bc+ppo_finetune\", \"timesteps\"].values[0]\n",
    "    / summary_df.loc[summary_df[\"type\"] == \"expert_baseline\", \"timesteps\"].values[0]\n",
    ")\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c73021",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "sns.barplot(data=summary_df, x=\"type\", y=\"timesteps\", palette=\"crest\", ax=ax)\n",
    "ax.set_title(\"Timesteps to Convergence\")\n",
    "ax.set_ylabel(\"Timesteps\")\n",
    "ax.set_xlabel(\"Run Type\")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"{:.0f}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b83142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not baseline_episode_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    sns.lineplot(\n",
    "        data=baseline_episode_df,\n",
    "        x=\"episode\",\n",
    "        y=\"steps\",\n",
    "        marker=\"o\",\n",
    "        ax=ax,\n",
    "        label=\"baseline\",\n",
    "    )\n",
    "    ax.set_title(\"Episode Steps (Baseline Expert)\")\n",
    "    ax.set_ylabel(\"Steps per Episode\")\n",
    "    ax.set_xlabel(\"Episode Index\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Baseline episode log missing; add JSONL to plot per-episode stats.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c8a15",
   "metadata": {},
   "source": [
    "## Findings\n",
    "- Fine-tuning converged in roughly 1k timesteps versus 100k for the expert baseline (≈99% reduction).\n",
    "- Metrics emitted in the current manifests are zeroed placeholders; once richer metrics are recorded, drop them into `summary_df` for more insightful plots.\n",
    "- Add additional sections for SNQI trends, reward curves, or policy evaluation results as new data becomes available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robot-sf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}